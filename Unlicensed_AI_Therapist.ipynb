{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/txxshalott/Unlicensed-AI-therapist/blob/main/Unlicensed_AI_Therapist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AI therapist prototype\n",
        "Author: Shalott Tam\\\n",
        "Date: April 18 - May 2\\\n",
        "Description: Not robust... use at your own risk\n",
        "\n"
      ],
      "metadata": {
        "id": "Vi-05msaJv2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTNeXg5i8G1H"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install python-dotenv\n",
        "\n",
        "import openai\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import drive\n",
        "drive.mount('//ColabNotebooks', force_remount=True)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_dotenv_path = '//ColabNotebooks/Colab Notebooks/Unlicensed_therapist/.env'\n",
        "load_dotenv(_dotenv_path)\n",
        "openai.api_key = os.getenv(\"API_KEY\")"
      ],
      "metadata": {
        "id": "-F5WseoXdCar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d40cd0-6b66-48c4-8e6c-4d2ad7f5ba57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0lxrTrtc7ypO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title The prototype { vertical-output: true }\n",
        "#@markdown Advice isnt terrible but a little wonky at times, use at your own risk (or dont cause it costs money too lol get real therapy)\n",
        "\n",
        "MAX_LEN = 200\n",
        "SUMMARY_TOKENS = 200\n",
        "response = \"\"\n",
        "lastResponse = \"\" #previous response from therapist\n",
        "summary = \"\"\n",
        "\n",
        "def therapist(prompt):\n",
        "    completions = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=70,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7, #higher = more creative\n",
        "        frequency_penalty = 0.7, #phrase, higher = try not to generate similar response\n",
        "    )\n",
        "    message = completions.choices[0].text.strip()\n",
        "    return message\n",
        "\n",
        "def summarize(prompt): #summarize, leaving the last few lines\n",
        "    completions = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=\"Summarize the following conversation: \" + prompt,\n",
        "        max_tokens=SUMMARY_TOKENS,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.4,\n",
        "    )\n",
        "    summary = completions.choices[0].text.strip()\n",
        "    return summary\n",
        "\n",
        "# Start convo\n",
        "starting = \"The following is a conversation with a helpful, wise, and friendly therapist. First, they help the client open up about their problems, after a while they start offering recommendations.\"\n",
        "prompt = starting\n",
        "print(\"Therapist: Hi there! I am your AI therapist, how are you feeling today?\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"bye\", \"goodbye\"]: # exit\n",
        "      print(\"Hope you feel better, bye!\")\n",
        "      break\n",
        "\n",
        "    prompt += \"\\nClient: \" + user_input\n",
        "\n",
        "    promptLen = len(prompt.split()) # approx tokens\n",
        "    if promptLen > MAX_LEN:\n",
        "      summary = summarize(prompt)  # summarize convo and make it the prompt\n",
        "\n",
        "      response = therapist(starting + summary + \"\\nTherapist: \")\n",
        "    else:\n",
        "      response = therapist(prompt + \"\\nTherapist: \")\n",
        "\n",
        "    ### IGNORE SECTION\n",
        "    # remove first sentence from response bc it always repeats what the user said\n",
        "    # but it only happens for a short while edit: ?? doesnt happen anymore\n",
        "    # if promptLen < 100:\n",
        "    #   print(\"response before token: \" + response)\n",
        "    #   sentences = sent_tokenize(response)\n",
        "    #   response = ' '.join(sentences[1:]) #slice 1 onward\n",
        "    ###\n",
        "\n",
        "    # if last sentence is not a complete sentence, ie last character it's not a punctuation\n",
        "    doc = nlp(response)\n",
        "    if doc[-1].is_alpha:\n",
        "      response = response[:response.rfind(\".\")] + \".\" # remove cut off sentence (taken from bryson)\n",
        "\n",
        "    lastResponse = \"\\nTherapist: \" + response\n",
        "    print(lastResponse)\n",
        "\n",
        "    prompt += response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SwcR9THMJsSu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MppUxM0v2Z9"
      },
      "outputs": [],
      "source": [
        "# data processing\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/ColabNotebooks/intentsData.json\")\n",
        "data = json.load(f)\n",
        "f.close()\n",
        "\n",
        "with open(\"output.jsonl\", \"w\") as f: # w means write mode\n",
        "  for intent in data[\"intents\"]:\n",
        "      for pattern in intent[\"patterns\"]: # each correspond to a response\n",
        "          for response in intent[\"responses\"]:\n",
        "              prompt = pattern.strip()\n",
        "              completion = response.strip()\n",
        "              json.dump({\"prompt\": prompt, \"completion\": completion}, f)\n",
        "              f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FBcnBMSg4Ja"
      },
      "outputs": [],
      "source": [
        "# wrap output text\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVsQ6hKilniu"
      },
      "outputs": [],
      "source": [
        "#@title Fine-tuned model\n",
        "#@markdown Trained with only a single data set, not robust and more expensive to run :[\\ { display-mode: \"form\" }\n",
        "#@markdown Tell the therapist how you're feeling below\n",
        "user_input = \"\" #@param {type:\"string\"}\n",
        "model_id = os.getenv(\"TUNED_MODEL_ID\")\n",
        "def thisSucksOmg(prompt):\n",
        "    completions = openai.Completion.create(\n",
        "        engine=model_id, #should be model_id after fine tuning\n",
        "        prompt=prompt,\n",
        "        max_tokens=20,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "        frequency_penalty = 0.5, #phrase, higher = try not to generate similar response\n",
        "        presence_penalty = 0, #word\n",
        "    )\n",
        "    message = completions.choices[0].text.strip()\n",
        "    return message\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"bye\", \"goodbye\"]:\n",
        "      print(\"Hope you feel better, bye!\")\n",
        "      break\n",
        "\n",
        "    prompt += \"\\nClient: \" + user_input\n",
        "    response = therapist(prompt + \"\\nTherapist: \")\n",
        "\n",
        "    lastResponse = \"\\nTherapist: \" + response\n",
        "    print(lastResponse)\n",
        "\n",
        "    prompt += response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may ignore everything below (failed attempts, debugging stuff)\n"
      ],
      "metadata": {
        "id": "tnGCX4e3dmZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeBa5Jtu7t8l"
      },
      "outputs": [],
      "source": [
        "### could not tune model on colab, did it in terminal instead\n",
        "\n",
        "def fine_tune_model(data):\n",
        "    response = openai.FineTune.create(\n",
        "        training_file=data,\n",
        "        model=\"davinci\",\n",
        "        prompt_loss_weight=0.1,\n",
        "        n_epochs=1,\n",
        "        batch_size=1,\n",
        "        suffix=\"out-of-tune\"\n",
        "    )\n",
        "\n",
        "    model_id = response.model.id\n",
        "    return model_id\n",
        "\n",
        "prompt = \"The following is a conversation with a helpful, creative, clever, and friendly therapist. At the beginning, they ask questions to learn more about the client's problems, emotional state, history. The goal is to help client open up about their problems.\"\n",
        "model_id = fine_tune_model(\"file name or smth\")\n",
        "print(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3svlYegqtXx"
      },
      "outputs": [],
      "source": [
        "### runs fine tuned model\n",
        "\n",
        "def runModel(modelName):\n",
        "  gen_response = modelName\n",
        "  prompt = \"The following is a conversation with a helpful, creative, clever, and friendly therapist. At the beginning, they ask questions to learn more about the client's problems, emotional state, history. The goal is to help client open up about their problems.\"\n",
        "  print(\"AI: Hi there! I am your AI therapist, how are you feeling today?\")\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"bye\", \"goodbye\"]:\n",
        "      print(\"Hope you feel better, bye!\")\n",
        "      break\n",
        "\n",
        "    prompt += \"\\nUser: \" + user_input\n",
        "    response = gen_response(prompt + \"\\nTherapist: \")\n",
        "    response = response[:response.rfind(\".\")] + \".\" # taken from bryson: remove cut off sentence\n",
        "\n",
        "    prompt += \"\\nTherapist: \" + response #therapist response\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6whLENZsxOKH"
      },
      "outputs": [],
      "source": [
        "### debug stuff\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the training data into a pandas DataFrame\n",
        "df = pd.DataFrame(formatted_data)\n",
        "\n",
        "# Check if the \"prompt\" column exists\n",
        "if 'prompt' not in df.columns:\n",
        "    print('Error: \"prompt\" column is missing in the training data')\n",
        "else:\n",
        "    print('Training data has a \"prompt\" column')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1VNksN_zJFd5AEl_Dw72F4gbcRXOiOCIZ",
      "authorship_tag": "ABX9TyN6C0A/qrctjhbkaD2k94fu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}